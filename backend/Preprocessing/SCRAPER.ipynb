{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam's PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(soup): \n",
    "    char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "    ret = []\n",
    "    body = BeautifulSoup(str(soup.find_all('p')), 'html.parser').text\n",
    "    body_clean = ''.join(e for e in body if e.lower() in char)\n",
    "    ret.append(body_clean)\n",
    "\n",
    "    if (body_clean != '[]' and body_clean != ''):\n",
    "        return ret\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam's PC\\AppData\\Local\\Temp\\ipykernel_32404\\1133812237.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n",
      "C:\\Users\\Sam's PC\\AppData\\Local\\Temp\\ipykernel_32404\\1133812237.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeURL(url): \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        if soup is not None:\n",
    "            scrape = getText(soup)\n",
    "            if scrape != False:\n",
    "                return scrape\n",
    "            else:\n",
    "                return 'No response'\n",
    "    except:\n",
    "        return 'No response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No response\n",
      "['Bidirectional Encoder Representations from Transformers BERT is a transformerbased machine learning technique for natural language processing NLP pretraining developed by Google BERT was created and published in  by Jacob Devlin and his colleagues from Google In  Google announced that it had begun leveraging BERT in its search engine and by late  it was using BERT in almost every Englishlanguage query  A  literature survey concluded that in a little over a year BERT has become a ubiquitous baseline in NLP experiments counting over  research publications analyzing and improving the model The original Englishlanguage BERT has two models  the BERTBASE  encoders with  bidirectional selfattention heads and  the BERTLARGE  encoders with  bidirectional selfattention heads Both models are pretrained from unlabeled data extracted from the BooksCorpus with M words and English Wikipedia with M words BERT is at its core a transformer language model  with a variable number of encoder layers and selfattention heads The architecture is almost identical to the original transformer implementation in Vaswani et al  BERT was pretrained on two tasks  language modeling  of tokens were masked and BERT was trained to predict them from context and next sentence prediction BERT was trained to predict if a chosen next sentence was probable or not given the first sentence As a result of the training process BERT learns contextual embeddings for words After pretraining which is computationally expensive BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks When BERT was published it achieved stateoftheart performance on a number of natural language understanding tasks The reasons for BERTs stateoftheart performance on these natural language understanding tasks are not yet well understood Current research has focused on investigating the relationship behind BERTs output as a result of carefully chosen input sequences analysis of internal vector representations through probing classifiers and the relationships represented by attention weights BERT has its origins from pretraining contextual representations including semisupervised sequence learning generative pretraining ELMo and ULMFit Unlike previous models BERT is a deeply bidirectional unsupervised language representation pretrained using only a plain text corpus Contextfree models such as wordvec or GloVe generate a single word embedding representation for each word in the vocabulary where BERT takes into account the context for each occurrence of a given word For instance whereas the vector for running will have the same wordvec vector representation for both of its occurrences in the sentences He is running a company and He is running a marathon BERT will provide a contextualized embedding that will be different according to the sentence On October   Google Search announced that they had started applying BERT models for English language search queries within the US On December   it was reported that BERT had been adopted by Google Search for over  languages In October  almost every single Englishbased query was processed by BERT The research paper describing BERT won the Best Long Paper Award at the  Annual Conference of the North American Chapter of the Association for Computational Linguistics NAACL BERT has been adapted for a number of domains by training the language model with domainspecific corpora such as']\n"
     ]
    }
   ],
   "source": [
    "print(ScrapeURL('https://www.google.com.au'))\n",
    "print(ScrapeURL('https://en.wikipedia.org/wiki/BERT_(language_model)'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6405ec6159b0547165700ce207a2b1c9d048e435bca4e5d6f2cd71ef647b6574"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
